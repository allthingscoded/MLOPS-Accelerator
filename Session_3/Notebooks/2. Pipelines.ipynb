{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating Model Build and Registration with SageMaker Pipelines\n",
    "\n",
    "> *This notebook is designed to work with the `Python 3 (Data Science)` kernel on SageMaker Studio.*\n",
    "\n",
    "Amazon SageMaker offers Machine Learning application developers and Machine Learning operations engineers the ability to orchestrate SageMaker jobs and author reproducible Machine Learning pipelines, deploy custom-build models for inference in real-time with low latency or offline inferences with Batch Transform, and track lineage of artifacts. You can institute sound operational practices in deploying and monitoring production workflows, deployment of model artifacts, and track artifact lineage through a simple interface, adhering to safety and best-practice paradigmsfor Machine Learning application development.\n",
    "\n",
    "The [SageMaker Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) service supports a SageMaker Machine Learning Pipeline Domain Specific Language (DSL), which is a declarative Json specification. This DSL defines a Directed Acyclic Graph (DAG) of pipeline parameters and SageMaker job steps. The [SageMaker Python Software Developer Kit (SDK)](https://sagemaker.readthedocs.io/en/stable/) streamlines the generation of the pipeline DSL using constructs that are already familiar to engineers and scientists alike.\n",
    "\n",
    "The [SageMaker Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html) is where trained models are stored, versioned, and managed. Data Scientists and Machine Learning Engineers can compare model versions, approve models for deployment, and deploy models from different AWS accounts, all from a single Model Registry. SageMaker enables customers to follow the best practices with ML Ops and getting started right. Customers are able to standup a full ML Ops end-to-end system with a single API call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Pipelines\n",
    "\n",
    "A SageMaker Pipeline defines a Directed Acyclic Graph (DAG) of steps and conditions to orchestrate SageMaker jobs and resource creation - supporting a [broad range of activities](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html) including:\n",
    "\n",
    "* **Processing Job steps** - A simplified, managed experience on SageMaker to run data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation.\n",
    "* **Training Job steps** - An iterative process that teaches a model to make predictions by presenting examples from a training dataset.\n",
    "* **Registering Models** - Creates a model package resource in the Model Registry that can be used to create deployable models in Amazon SageMaker.\n",
    "* **Creating Model steps** - Create a model for use in transform steps or later publication as an endpoint.\n",
    "* **Transform Job steps** - A batch transform to preprocess datasets to remove noise or bias that interferes with training or inference from your dataset, get inferences from large datasets, and run inference when you don't need a persistent endpoint.\n",
    "* **Conditional step execution** - Provides conditional execution of branches in a pipeline.\n",
    "* **Parameterized Pipeline executions** - Allows pipeline executions to vary by supplied parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline that we'll create in this example follows a typical Machine Learning Application pattern of pre-processing, training, evaluation, and conditional model registration and publication, if the quality of the model is sufficient.\n",
    "\n",
    "![](imgs/sm-pipelines.png \"A typical ML Application pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the various Python libraries we'll use in the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import os\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # The general-purpose AWS SDK for Python\n",
    "import sagemaker  # High-level Python SDK for Amazon SageMaker\n",
    "\n",
    "print(f\"sagemaker SDK v{sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can connect to AWS services and **configure**:\n",
    "\n",
    "- The [Amazon S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html) and prefix to use for storing data and artifacts.\n",
    "- The [IAM role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) ARN to be used for accessing data and other resources.\n",
    "- Some additional **names and prefixes** for the pipeline and model package resources the notebook will create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_session = sagemaker.Session()\n",
    "pipeline_session = sagemaker.workflow.pipeline_context.PipelineSession()\n",
    "s3 = boto3.resource(\"s3\")  # Amazon S3\n",
    "\n",
    "bucket = sm_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pipelines-churn\"\n",
    "print(f\"Saving S3 data to: s3://{bucket}/{prefix}\")\n",
    "\n",
    "base_job_prefix=\"CustomerChurn\"\n",
    "model_package_group_name = \"ChurnModelPackageGroup-v1\"\n",
    "pipeline_name = \"ChurnPipeline-v1\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Using IAM role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gather the data\n",
    "\n",
    "This example will use a synthetic customer churn dataset from the public `sagemaker-sample-files` bucket: The same as the previous Autopilot notebook. In a real scenario you would access your own data, typically on your own S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data from s3 to our local folder\n",
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt ./data/churn.txt\n",
    "\n",
    "# Copy the file to our own S3 bucket with a csv extension\n",
    "raw_data_key = f\"{prefix}/data/RawData.csv\"\n",
    "s3.Bucket(bucket).Object(raw_data_key).upload_file(\"./data/churn.txt\")\n",
    "raw_data_s3uri = f\"s3://{bucket}/{raw_data_key}\"\n",
    "\n",
    "print(f\"\\nRaw data loaded to:\\n{raw_data_s3uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Processing step for feature engineering\n",
    "\n",
    "For our use case we keep the actual pre-processing code within a separate python file [preprocess.py](preprocess.py). This script will drop irrelevant columns, encode categorical values, create a target value and split the data between test, train and validation.\n",
    "\n",
    "> ⚠️ **Note:** Below we use these *Pipeline parameter objects* in place of actual values - for example:\n",
    ">\n",
    "> 1. `processing_instance_type`\n",
    "> 1. `processing_instance_count`, and\n",
    "> 1. `input_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import CacheConfig, ProcessingStep\n",
    "\n",
    "# 1. parameters for the pre-processing stage\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger( name=\"ProcessingInstanceCount\", default_value=1,)\n",
    "processing_instance_type = ParameterString( name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\",)\n",
    "input_data = ParameterString( name=\"InputDataUrl\", default_value=raw_data_s3uri, )\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=processing_instance_type,  # (Pipeline parameter)\n",
    "    instance_count=processing_instance_count,  # (Pipeline parameter)\n",
    "    # base_job_name=f\"{base_job_prefix}/sklearn-CustomerChurn-preprocess\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "step_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name=\"raw\",\n",
    "            source=input_data,  # (Pipeline parameter)\n",
    "            destination=\"/opt/ml/processing/input/raw\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"./preprocess.py\",\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"CustomerChurnProcess\",\n",
    "    step_args=step_args,\n",
    "    cache_config=CacheConfig(enable_caching=True, expire_after=\"T2H\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training step for generating model artifacts\n",
    "\n",
    "On this step we will define [XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) as our algorithm of choice. This example uses **both**:\n",
    "\n",
    "- **Pipeline input parameters** which can be customized at execution time (`training_instance_type`, `training_max_depth`), and\n",
    "- **Dependencies** from the previous processing step (`step_process.properties.{***}`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# 2. parameters for the training stage. No need to import ParameterString since it was imported earlier.\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\",)\n",
    "training_max_depth = ParameterString(name=\"TrainingMaxDepth\", default_value=\"5\",)        \n",
    "\n",
    "model_path = f\"s3://{bucket}/{base_job_prefix}/CustomerChurnTrain\"\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",  # we are using the Sagemaker built in xgboost algorithm\n",
    "    region=sm_session.boto_session.region_name,  # e.g. 'us-east-1'\n",
    "    version=\"1.3-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=training_instance_type,  # (Pipeline parameter)\n",
    ")\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=training_instance_type,  # (Pipeline parameter)\n",
    "    instance_count=1,\n",
    "    output_path=model_path,\n",
    "    base_job_name=f\"{base_job_prefix}/CustomerChurn-train\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    num_round=50,\n",
    "    max_depth=training_max_depth,  # (Pipeline parameter)\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    ")\n",
    "\n",
    "step_args = xgb_train.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            # Use an output from one step as an input (dependency) in another:\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            # Use an output from one step as an input (dependency) in another:\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "step_train = TrainingStep(\n",
    "    name=\"CustomerChurnTrain\",\n",
    "    step_args=step_args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Processing step for evaluation\n",
    "\n",
    "This step uses one parameter which we can customize at execution time (`processing_instance_type`), and again references outputs from other steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "# processing_instance_type parameter was already defined in stage 2 but it is reused here\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,  # (Pipeline parameter)\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{base_job_prefix}/script-CustomerChurn-eval\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "step_args = script_eval.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[ ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"), ],\n",
    "    code=\"./evaluate.py\",\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"CustomerChurnEval\",\n",
    "    step_args=step_args,\n",
    "    property_files=[evaluation_report],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Register model step that will be conditionally executed\n",
    "\n",
    "Pipelines allow us to have conditional steps, to register models into our registry and optionally to publish these to staging. A common usage is to check a minimum threshold (using F1 in our case) and only publish into our registry if it is higher than a given threshold (0.8 in our case). \n",
    "\n",
    "This step uses one parameter which we can customize at execution time: \n",
    "`model_approval_status`\n",
    "\n",
    "> ▶️ **Question:** *Which other variables could we add from this section as parameter for the pipeline?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker import Model\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "# Parameter for the model register stage\n",
    "# (ModelApprovalStatus can be set to a default of \"Approved\" if you don't want manual approval)\n",
    "model_approval_status = ParameterString( name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\",)\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    image_uri=xgb_train.image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "\n",
    "# Register model step that will be conditionally executed\n",
    "register_args = model.register(\n",
    "    # estimator=xgb_train,\n",
    "    # model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "step_register = ModelStep(\n",
    "    name=\"CustomerChurnRegisterModel\",\n",
    "    step_args=register_args,\n",
    ")\n",
    "\n",
    "# Condition step for evaluating model quality and branching execution\n",
    "cond_register = ConditionGreaterThanOrEqualTo(  # You can change the condition here\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        # This should follow the structure of your report_dict defined in the evaluate.py file:\n",
    "        json_path=\"binary_classification_metrics.f1.value\",\n",
    "    ),\n",
    "    right=0.8,  # TODO: add as a pipeline parameter\n",
    ")    \n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"CustomerChurnAccuracyCond\",\n",
    "    conditions=[cond_register],\n",
    "    if_steps=[step_register],\n",
    "    else_steps=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Construct Pipeline \n",
    "\n",
    "With the steps and pipeline-level parameters all defined, we're ready to create the overall [Pipeline](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#pipeline) and register it with SageMaker.\n",
    "\n",
    "> ⚠️ **Remember:** if you added any additional parameters, you will need to add them to this pipeline definition as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_max_depth,            \n",
    "        training_instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond],\n",
    "    sagemaker_session=sm_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline definition is not actually submitted to the SageMaker service until we `upsert()` it. The role passed in here with the definition will be used by the workflow service to create all the jobs defined in the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `upsert()`ing your pipeline, it should be registered with the SageMaker APIs and ready to use. For example, you should see the pipeline available in the SageMaker Studio UI through the *Resources > Pipelines* sidebar menu:\n",
    "\n",
    "![](imgs/sm-pipeline-registered.png \"Screenshot of SageMaker Studio UI showing registered churn pipeline\")\n",
    "\n",
    "### 7. Run the Pipeline\n",
    "\n",
    "Before we start experimenting with the execution parameters and UI though, let's see how to start and check the pipeline with default parameters from code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Pipeline Operations: examining and waiting for pipeline execution\n",
    "\n",
    "Starting a pipeline creates an **execution** instance, which can be queried to list the steps in the execution and find out more about its progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wait for the execution by invoking `wait()` on the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list the execution steps to check out the status and artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Parameterized Executions\n",
    "\n",
    "We can run additional executions of the pipeline specifying different pipeline parameters. The parameters argument is a dictionary whose names are the parameter names, and whose values are the primitive values to use as overrides of the defaults.\n",
    "\n",
    "Of particular note, based on the performance of the model, we may want to kick off another pipeline execution, but this time on a compute-optimized instance type and set the model approval status automatically be \"Approved\". This means that the model package version generated by the `RegisterModel` step will automatically be ready for deployment through CI/CD pipelines, such as with SageMaker Projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        ProcessingInstanceType=\"ml.c5.xlarge\",\n",
    "        ModelApprovalStatus=\"Approved\",\n",
    "        TrainingMaxDepth=\"6\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Pipelines\n",
    "\n",
    "▶️ **Run** the previous execution from the **UI in Sagemaker Studio** instead of using a Notebook.\n",
    "\n",
    "▶️ **Add** the minimum F1 score for model registration as a **parameter** to the pipeline, instead of the hard-coded `0.8` value used above. Check you can update and run your pipeline successfully to override the threshold!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring and Comparing Experiments in SageMaker Studio\n",
    "\n",
    "So far we've seen that SageMaker Pipelines are useful for automating and orchestrating multi-step machine learning workflows - but what tools are available for exploring the results of these different experiments, and preparing for production deployment?\n",
    "\n",
    "Two particularly relevant features here are:\n",
    "\n",
    "- [SageMaker Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html) - which supports **approval workflows** tracking **model versions** with additional metrics and metadata (such as for model quality, bias, and data drift)\n",
    "- [SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) - through which we can organize, explore details of, and compare, past jobs through the Studio UI.\n",
    "\n",
    "Like the Pipelines UI highlighted earlier, these tools and others are available through the \"SageMaker Resources\" tab in SageMaker Studio:\n",
    "\n",
    "![](imgs/sm-resources-menu.png \"SageMaker Studio screenshot showing resources sidebar menu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing and Comparing Metrics in Model Registry\n",
    "\n",
    "Our example pipeline should have created a \"Model Package Group\" already visible in the model registry (Per `model_package_group_name` above).\n",
    "\n",
    "If you've run the pipeline with the original and modified `TrainingMaxDepth` parameters, you should see at least 2 successful executions... And with these parameters, both runs should have exceeded the `CustomerChurnAccuracyCond` threshold for being added to the model registry.\n",
    "\n",
    "▶️ **Shift-click** to select *two or more* Model Versions in the Model Registry screen, and then **Right-click** to see the *Compare model versions* option\n",
    "\n",
    "![](imgs/sm-registry-compare.png \"SM Model registry with multiple model versions selected and right-click menu open\")\n",
    "\n",
    "For models like the ones in our example which are [registered](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModelPackage.html) with [Model Quality Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html), you'll be able to see a comparison of model metrics - and even charts like [confusion matrices](https://en.wikipedia.org/wiki/Confusion_matrix) and [ROC curves](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) if these are present in the model metadata.\n",
    "\n",
    "▶️ **Which** of your model versions seemed to perform best by the different metrics? Was there much difference?\n",
    "\n",
    "You can also update model versions' **approval status** through the Model Registry UI, which can be used to trigger deployment of your models to different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Trial Components in SageMaker Experiments\n",
    "\n",
    "In SageMaker Experiments:\n",
    "\n",
    "- A **`Trial Component`** corresponds to a particular job or stage - for example a previously executed training or pre-processing job.\n",
    "- A **`Trial`** is a collection of multiple Trial Components in one end-to-end attempt (for example, pre-processing plus model training plus evaluation)\n",
    "- An **`Experiment`** is a collection of different, comparable, *Trials* which attempted to tackle the same problem.\n",
    "\n",
    "These map automatically to SageMaker pipeline definitions: A particular pipeline execution automatically creates a **`Trial`**, and a defined pipeline is an overall **`Experiment`**. Similarly, you'll see that SageMaker Autopilot automatically generates Experiments as it runs. You can also manually organize SageMaker jobs you create into your own Trials and Experiments.\n",
    "\n",
    "In the SageMaker Experiments UI, double-clicking drills down through this Experiment > Trial > Component hierarchy - and at the top level you'll also see `Unassigned trial components`, where any training jobs (or processing, etc) that haven't yet been tagged to Experiments are available.\n",
    "\n",
    "![](imgs/sm-execution-trial-components.png \"Trial components list for a specific pipeline execution\")\n",
    "\n",
    "> ⚠️ **Different right-click options and double-click behaviours** are available at different points in the hierarchy, and for different job types.\n",
    ">\n",
    "> For example:\n",
    ">\n",
    "> - Right-clicking on an individual training job will show `Open Debugger for insights` as well as `Open in trial details`.\n",
    "> - Right-clicking on an Autopilot Experiment will show `Describe AutoML Job` in addition to `Open in trial component list`.\n",
    "\n",
    "▶️ **Find** A SageMaker Training Job in your existing experiments and **double-click** on it to open the job details: You should see customizable charts as well as metrics and other job metadata.\n",
    "\n",
    "▶️ **Right-click** Your Training Job in Experiments to `Open Debugger for insights`. Can you find performance recommendations for your training job, and detailed resource utilization metrics per-node in the job?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Jobs in SageMaker Experiments\n",
    "\n",
    "Drilling in to individual Trial Components is fine for exploring particular training jobs, but can we compare metrics and charts across multiple jobs at once? Yes you can!\n",
    "\n",
    "▶️ **Right-click** on your pipeline's top-level **Experiment** and select `Open in trial component list` to open the *list of multiple trial components* run by the pipeline\n",
    "\n",
    "(You can also do this at Trial level in the hierarchy, but typically you'll want to be comparing across trials, rather than jobs within one trial)\n",
    "\n",
    "![](imgs/sm-pipeline-tcs-list.png \"Trial Components list for the demo pipeline, showing multiple training jobs\")\n",
    "\n",
    "▶️ **Shift-click** to select multiple *training job* components, and then click \"Add Chart\" to build a composite chart including the multiple training jobs.\n",
    "\n",
    "\n",
    "▶️ **Configure** your chart via the right sidebar menu, to set up:\n",
    "\n",
    "- `Summary statistics` from each training job (not time series)\n",
    "- `Scatter plot` chart type\n",
    "- `max_depth` X axis\n",
    "- `validation:logloss_last` Y axis\n",
    "- `trialComponentName` Color\n",
    "\n",
    "You should be able to set up a comparative scatter plot illustrating the impact of max_depth on the final validation loss (lower = better) in model training, similar to the chart shown below.\n",
    "\n",
    "![](imgs/sm-tcs-comparison-chart.png \"Screenshot of scatter chart comparing multiple training jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this section we've just scratched the surface of the tools available in SageMaker Studio to help manage your experiments, model versions, and model building pipelines.\n",
    "\n",
    "The metrics you report during training jobs and tag against your Model Registry model versions enable these features. You can add additional metrics and hyperparameters during model training, and of course define additional steps and parameters for your model building pipelines.\n",
    "\n",
    "You can find lots more information about all these topics, and additional MLOps features like [SageMaker Project Templates](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.html), in the [SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html)."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
