{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with Amazon SageMaker Autopilot\n",
    "\n",
    "> *This notebook is designed to work with the `Python 3 (Data Science)` kernel on SageMaker Studio. With the example dataset, the default `ml.t3.medium` (2 vCPU + 4 GiB) instance type should work well.*\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <p>▶️ <b>Workshop fast-start</b></p>\n",
    "    <p>To get started with this workshop, go to the <a href=\"#Quickstart\"><u>quick-start box below (link)</u></a> and follow the instructions to run the first set of cells.</p>\n",
    "    <p>This will help ensure your Autopilot job is started promptly and you see results during the session</p>\n",
    "</div>\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Settingup)\n",
    "1. [Autopilot Results](#Results)\n",
    "1. [Evaluate Top Candidates](#Evaluation)\n",
    "1. [Cleanup](#Cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction <a name=\"Intro\"></a>\n",
    "\n",
    "[Amazon SageMaker Autopilot](https://aws.amazon.com/sagemaker/autopilot/) is an automated machine learning (commonly referred to as AutoML) solution for tabular datasets. You can use SageMaker Autopilot in different ways:\n",
    "\n",
    "- On autopilot (hence the name) or with human guidance\n",
    "- Through the no-code UI (in SageMaker Studio), or via the AWS SDKs.\n",
    "\n",
    "This notebook will demonstrate Autopilot via code with the high-level [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) - training, evaluating, and deploying an ML model for **churn prediction**.\n",
    "\n",
    "Losing customers is costly for any business.  Identifying unhappy customers early on gives you a chance to offer them incentives to stay.  This notebook describes using machine learning for the automated identification of unhappy customers, also known as customer churn prediction.\n",
    "\n",
    "We use an example of churn that is familiar to many: leaving a mobile phone operator.  Seems like I can always find fault with my provider du jour! And if my provider knows that I’m thinking of leaving, it can offer timely incentives – for example a new phone upgrade or activating a new feature.  Incentives like this could keep me around, are often much more cost effective for the business than losing and reacquiring a customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "Before getting started, we'll check & upgrade a few installed library versions to avoid some past incompatibilities (see [numpy#18355](https://github.com/numpy/numpy/issues/18355) and [aiobotocore#905](https://github.com/aio-libs/aiobotocore/issues/905)).\n",
    "\n",
    "> ⚠️ **Note:** If you have any other notebooks running in the same Studio \"app\" (same kernel and instance type) that have already imported these libraries into memory, you might see unexpected errors and need to restart those notebook kernels after this install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install \"pandas>=1.0.5\" \"s3fs>=2022.01.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the right library versions installed, we can import the ones we'll use in the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import io\n",
    "import json\n",
    "from pprint import pprint\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # The general-purpose AWS SDK for Python\n",
    "import matplotlib.pyplot as plt  # Graph plotting\n",
    "import numpy as np  # Numeric & matrix utilities\n",
    "import pandas as pd  # DataFrame (tabular data) tools\n",
    "import sagemaker  # High-level Python SDK for Amazon SageMaker\n",
    "from sagemaker import AutoML\n",
    "from sklearn import metrics as skmetrics  # Model evaluation metrics\n",
    "\n",
    "print(f\"sagemaker SDK v{sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can connect to AWS services and **configure**:\n",
    "\n",
    "- The [Amazon S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html) and prefix to use for storing data and artifacts.\n",
    "- The [IAM role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) ARN to be used for accessing data and other resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "sm_session = sagemaker.Session() \n",
    "\n",
    "bucket = sm_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-autopilot-churn\"\n",
    "print(f\"Saving S3 data to: s3://{bucket}/{prefix}\")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Using IAM role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "Mobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes–after all, predicting the future is tricky business! But I’ll also show how to deal with prediction errors.\n",
    "\n",
    "The dataset we will use is synthetically generated, but indictive of the types of features you'd see in this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt ./data/churn.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect your Dataset\n",
    "\n",
    "Before you run Autopilot on the dataset, first perform a check of the dataset to make sure that it has no obvious errors. The Autopilot process can take long time, and it's generally a good practice to inspect the dataset before you start a job. This particular dataset is small, so you can simply load it into notebook memory using Pandas.\n",
    "\n",
    "If you have a larger dataset that won't fit into notebook memory, you could inspect the dataset offline using a big data analytics tool like Apache Spark. [Deequ](https://github.com/awslabs/deequ) is a library built on top of Apache Spark that can be helpful for performing checks on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./data/churn.txt\")\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By modern standards, it’s a relatively small dataset, with only 5,000 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are:\n",
    "\n",
    "- `State`: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ\n",
    "- `Account Length`: the number of days that this account has been active\n",
    "- `Area Code`: the three-digit area code of the corresponding customer’s phone number\n",
    "- `Phone`: the remaining seven-digit phone number\n",
    "- `Int’l Plan`: whether the customer has an international calling plan: yes/no\n",
    "- `VMail Plan`: whether the customer has a voice mail feature: yes/no\n",
    "- `VMail Message`: presumably the average number of voice mail messages per month\n",
    "- `Day Mins`: the total number of calling minutes used during the day\n",
    "- `Day Calls`: the total number of calls placed during the day\n",
    "- `Day Charge`: the billed cost of daytime calls\n",
    "- `Eve Mins, Eve Calls, Eve Charge`: the billed cost for calls placed during the evening\n",
    "- `Night Mins`, `Night Calls`, `Night Charge`: the billed cost for calls placed during nighttime\n",
    "- `Intl Mins`, `Intl Calls`, `Intl Charge`: the billed cost for international calls\n",
    "- `CustServ Calls`: the number of calls placed to Customer Service\n",
    "- `Churn?`: whether the customer left the service: true/false\n",
    "\n",
    "The last attribute, `Churn?`, is known as the target attribute–the attribute that we want the ML model to predict. We'll save this information to use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure which column is the target and which of the values is 'true':\n",
    "target_attribute_name = \"Churn?\"\n",
    "target_attribute_true_value = \"True.\"\n",
    "\n",
    "# Print out summary of the target column:\n",
    "target_attribute_values_counts = raw_data[target_attribute_name].value_counts()\n",
    "print(\"Target column value counts:\")\n",
    "print(target_attribute_values_counts)\n",
    "print(f\"\\nTarget 'True' value is: '{target_attribute_true_value}'\")\n",
    "\n",
    "# Check the configuration is good:\n",
    "assert target_attribute_true_value in target_attribute_values_counts, (\n",
    "    f\"Couldn't find '{target_attribute_true_value}' in target column!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reserve some data for final testing of the model\n",
    "\n",
    "Divide the data into training and testing splits. The training split is used by SageMaker Autopilot. The testing split is reserved to perform inference using the suggested model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = raw_data.sample(frac=0.8, random_state=200)\n",
    "\n",
    "test_data = raw_data.drop(train_data.index)\n",
    "\n",
    "test_data_no_target = test_data.drop(columns=[target_attribute_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save these train and test data splits to csv files locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/train_data.csv\"\n",
    "train_data.to_csv(train_file, index=False, header=True)\n",
    "\n",
    "test_file = \"data/test_data.csv\"\n",
    "test_data_no_target.to_csv(test_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setting up the SageMaker Autopilot Job<a name=\"Settingup\"></a>\n",
    "\n",
    "We'll use the [`AutoML` class](https://sagemaker.readthedocs.io/en/stable/api/training/automl.html#sagemaker.automl.automl.AutoML) from the SageMaker Python SDK to invoke Autopilot to find the best ML pipeline for this dataset. \n",
    "\n",
    "The required inputs for invoking a Autopilot job are:\n",
    "\n",
    "* Local or s3 location for input dataset (if local, the dataset will be uploaded to S3)\n",
    "* Name of the column of the dataset you want to predict (`Churn?` in this case) \n",
    "* An IAM role\n",
    "\n",
    "You can find more information about input dataset format requirements for Autopilot [in the SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-datasets-problem-types.html).\n",
    "\n",
    "> ⏰ **Note:** Below we override the `max_candidates` for Autopilot to generate and train to be significantly lower than the default (250). This allows us to speed up the job run significantly for the workshop (from ~4 hours to ~20 minutes), at the expense of the accuracy of the output model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique job name (in case this experiment is run multiple times)\n",
    "timestamp_suffix = time.strftime(\"%d-%H-%M-%S\", time.gmtime())\n",
    "base_job_name = \"automl-churn-sdk-\" + timestamp_suffix\n",
    "\n",
    "automl = AutoML(\n",
    "    role=role,\n",
    "    target_attribute_name=target_attribute_name,\n",
    "    base_job_name=base_job_name,\n",
    "    sagemaker_session=sm_session,\n",
    "    max_candidates=5,  # Maximum candidates Autopilot will generate (default 250)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the type of problem you want to solve with your dataset (`Regression, MulticlassClassification, BinaryClassification`) with the `problem_type` keyword argument. In case you are not sure, SageMaker Autopilot will infer the problem type based on statistics of the target column (the column you want to predict). \n",
    "\n",
    "Because the target attribute, ```Churn?```, is binary, our model will be performing binary prediction, also known as binary classification. In this example we will let AutoPilot infer the type of problem for us.\n",
    "\n",
    "You have the option to limit the running time of a SageMaker Autopilot job by providing either the maximum number of pipeline evaluations or \"candidates\" (one pipeline evaluation is called a `Candidate` because it generates a candidate model) or providing the total time allocated for the overall Autopilot job. Under default settings, this job takes about four hours to run. This varies between runs because of the exploratory nature of the process Autopilot uses to find optimal training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching the SageMaker Autopilot Job<a name=\"Launching\"></a>\n",
    "\n",
    "You can now launch the Autopilot job by calling the `fit` method of the `AutoML` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.fit(train_file, job_name=base_job_name, wait=False, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note: Using a previous Autopilot Job**\n",
    "> \n",
    "> If you want to retrieve a previous Autopilot job or an Autopilot job launched outside of this notebook, such as from the SageMaker Studio UI, from the CLI, etc, you can use the following lines to prior to the next cell. If you are using a different dataset, you must also override the following variables defined in the [Data](#Data) section in order to run the batch jobs and perform the analysis: `test_data`, `test_data_no_target`, `test_file`, `target_attribute_name`, `target_attribute_values`, and `target_attribute_true_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automl = AutoML.attach(\"automl-churn-sdk-21-09-03-39\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking SageMaker Autopilot Job Progress<a name=\"Tracking\"></a>\n",
    "SageMaker Autopilot job consists of the following high-level steps : \n",
    "* Analyzing Data, where the dataset is analyzed and Autopilot comes up with a list of ML pipelines that should be tried out on the dataset. The dataset is also split into train and validation sets.\n",
    "* Feature Engineering, where Autopilot performs feature transformation on individual features of the dataset as well as at an aggregate level.\n",
    "* Model Tuning, where the top performing pipeline is selected along with the optimal hyperparameters for the training algorithm (the last stage of the pipeline). \n",
    "\n",
    "We can use the `describe_auto_ml_job` method to check the status of our SageMaker Autopilot job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"JobStatus - Secondary Status\")\n",
    "print(\"----------------------------\", end=\"\")\n",
    "\n",
    "job_run_status = None\n",
    "secondary_status = None\n",
    "\n",
    "while job_run_status not in (\"Failed\", \"Completed\", \"Stopped\"):\n",
    "    if job_run_status is not None:\n",
    "        time.sleep(60)\n",
    "    describe_response = automl.describe_auto_ml_job()\n",
    "    new_status = describe_response[\"AutoMLJobStatus\"]\n",
    "    new_secondary_status = describe_response[\"AutoMLJobSecondaryStatus\"]\n",
    "    \n",
    "    if new_status == job_run_status and new_secondary_status == secondary_status:\n",
    "        print(\".\", end=\"\")\n",
    "    else:\n",
    "        print(f\"\\n{new_status} - {new_secondary_status} \", end=\"\")\n",
    "        job_run_status = new_status\n",
    "        secondary_status = new_secondary_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Quickstart\"></a>\n",
    "<div class=\"alert alert-info\">\n",
    "    <p>▶️ <b>Workshop fast-start</b></p>\n",
    "    <p>To get started quickly with this workshop:</p>\n",
    "    <ol>\n",
    "        <li><b>Click</b> here to select this cell (a blue bar will appear to the left of it)</li>\n",
    "        <li>In the menu bar above, select <b><i>Run</i></b> and <b><i>Run All Above Selected Cell</i></b></li>\n",
    "    </ol>\n",
    "    <p>This will automatically run all code cells above here, to make sure you get your AutoML job kicked off quickly and see results during the workshop.</p>\n",
    "    <p>Once that's done, head back up to the <a href=\"#Intro\"><u>Introduction</u></a> to explore the code.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ▶️ **While the job runs:** Are you able to re-create the same Autopilot job parameters using the SageMaker Studio UI?\n",
    ">\n",
    "> You'll find *Create Autopilot Experiment* under the `SageMaker Resources > Experiments and trials` Section of the left sidebar menu. ⬅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "---\n",
    "## Describing the SageMaker Autopilot Job Results <a name=\"Results\"></a>\n",
    "\n",
    "We can use the `describe_auto_ml_job` method to look up the best candidate generated by the SageMaker Autopilot job. This notebook demonstrate end-to-end Autopilot so that we have a already initialized `automl` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_candidate = automl.describe_auto_ml_job()[\"BestCandidate\"]\n",
    "best_candidate_name = best_candidate[\"CandidateName\"]\n",
    "pprint(best_candidate)\n",
    "print(\"\\n\")\n",
    "print(\"CandidateName: \" + best_candidate_name)\n",
    "print(\n",
    "    \"FinalAutoMLJobObjectiveMetricName: \"\n",
    "    + best_candidate[\"FinalAutoMLJobObjectiveMetric\"][\"MetricName\"]\n",
    ")\n",
    "print(\n",
    "    \"FinalAutoMLJobObjectiveMetricValue: \"\n",
    "    + str(best_candidate[\"FinalAutoMLJobObjectiveMetric\"][\"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to some randomness in the algorithms involved, different runs will provide slightly different results, but accuracy will be around or above $93\\%$, which is a good result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Top Candidates\n",
    "\n",
    "In addition to the `best_candidate`, we can also explore the other top candidates generated by SageMaker Autopilot. \n",
    "\n",
    "We use the `list_candidates` method to see our other top candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Autopilot candidates to evaluate and run batch transform jobs.\n",
    "# Make sure that you do not put a larger TOP_N_CANDIDATES than the Batch Transform limit for ml.m5.xlarge instances in your account.\n",
    "TOP_N_CANDIDATES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = automl.list_candidates(\n",
    "    sort_by=\"FinalObjectiveMetricValue\", sort_order=\"Descending\", max_results=TOP_N_CANDIDATES\n",
    ")\n",
    "candidates_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Candidate Name\": c[\"CandidateName\"],\n",
    "        \"Objective Metric Name\": c[\"FinalAutoMLJobObjectiveMetric\"][\"MetricName\"],\n",
    "        \"Objective Metric Value\": c[\"FinalAutoMLJobObjectiveMetric\"][\"Value\"],\n",
    "    }\n",
    "    for c in candidates\n",
    "])\n",
    "\n",
    "candidates_df.sort_values([\"Objective Metric Value\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluate Top Candidates <a name=\"Evaluation\"></a>\n",
    "\n",
    "Once our SageMaker Autopilot job has finished, we can start running inference on the top candidates. In SageMaker, you can perform inference in two ways: online endpoint inference or batch transform inference. Lets focus on batch transform inference.\n",
    "\n",
    "We'll perform batch transform on our top candidates and analyze some custom metrics from our top candidates' prediction results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data for Transform Jobs\n",
    "\n",
    "We'll use the `test_data` which we defined when we split out data in train and test splits. We need to upload this data to S3. As a refresher, here's `test_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_transform = sm_session.upload_data(path=test_file, bucket=bucket, key_prefix=prefix)\n",
    "print(\"Uploaded transform data to {}\".format(input_data_transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize the Inference Response\n",
    "\n",
    "For classification problem types, the inference containers generated by SageMaker Autopilot allow you to select the response content for predictions. Valid inference response content are defined below for binary classification and multiclass classification problem types.\n",
    "\n",
    "- `'predicted_label'` - predicted class\n",
    "- `'probability'` - In binary classification, the probability that the result is predicted as the second or `True` class in the target column. In multiclass classification, the probability of the winning class.\n",
    "- `'labels'` - list of all possible classes\n",
    "- `'probabilities'` - list of all probabilities for all classes (order corresponds with `'labels'`)\n",
    "\n",
    "By default the inference contianers are configured to generate the `'predicted_label'`.\n",
    "\n",
    "In this example we use `‘predicted_label’` and `‘probability’` to demonstrate how to evaluate the models with custom metrics. For the Churn dataset, the second or `True` class is the string`'True.'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_response_keys = [\"predicted_label\", \"probability\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Models and Tranform Estimators\n",
    "\n",
    "Let's create our Models and Batch Transform Estimators using the `create_model` method. We can specify our inference response using the `inference_response_keys` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_transform_output_path = \"s3://{}/{}/inference-results/\".format(bucket, prefix)\n",
    "\n",
    "transformers = []\n",
    "\n",
    "for candidate in candidates:\n",
    "    model = automl.create_model(\n",
    "        name=candidate[\"CandidateName\"],\n",
    "        candidate=candidate,\n",
    "        inference_response_keys=inference_response_keys,\n",
    "    )\n",
    "\n",
    "    output_path = s3_transform_output_path + candidate[\"CandidateName\"] + \"/\"\n",
    "\n",
    "    transformers.append(\n",
    "        model.transformer(\n",
    "            instance_count=1,\n",
    "            instance_type=\"ml.m5.xlarge\",\n",
    "            assemble_with=\"Line\",\n",
    "            output_path=output_path,\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"Setting up {} Batch Transform Jobs in `transformers`\".format(len(transformers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Transform Jobs\n",
    "\n",
    "Let's start all the transform jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transformer in transformers:\n",
    "    transformer.transform(\n",
    "        data=input_data_transform, split_type=\"Line\", content_type=\"text/csv\", wait=False\n",
    "    )\n",
    "    print(\"Starting transform job {}\".format(transformer._current_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wait for our transform jobs to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_job_names = [t.latest_transform_job.name for t in transformers]\n",
    "print(f\"Polling {len(running_job_names)} transform jobs \", end=\"\")\n",
    "\n",
    "while len(running_job_names):\n",
    "    time.sleep(30)\n",
    "    remaining = []\n",
    "    for name in running_job_names:\n",
    "        job_desc = sm_session.describe_transform_job(transformers[0].latest_transform_job.name)\n",
    "        job_status = job_desc[\"TransformJobStatus\"]\n",
    "        if \"fail\" in job_status.lower():\n",
    "            print(job_desc)\n",
    "            raise RuntimeError(f\"Transform job {name} failed with status {job_status}\")\n",
    "        elif job_status == \"Completed\":\n",
    "            print(f\"\\nJob {name} completed \", end=\"\")\n",
    "        else:\n",
    "            remaining.append(name)\n",
    "    if len(remaining) != len(running_job_names):\n",
    "        print(f\"\\n{len(remaining)} jobs still running \", end=\"\")\n",
    "    else:\n",
    "        print(\".\", end=\"\")\n",
    "    running_job_names = remaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Inference Results\n",
    "\n",
    "Now we analyze our inference results. Note that Pandas is able to read files direct from `s3://...` URIs.\n",
    "\n",
    "> ⚠️ **Note:** Since this is a binary classification task, the `probability` output from Autopilot is the probability of the `True.` label, **not** of whichever label was assigned: So for example the probabilities of `True.` records should all be near 1 and those of `False.` should be near 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for transformer in transformers:\n",
    "    output_path = transformer.output_path + \"test_data.csv.out\"\n",
    "    print(output_path)\n",
    "    predictions.append(\n",
    "        pd.read_csv(\n",
    "            transformer.output_path + \"test_data.csv.out\",\n",
    "            header=None,\n",
    "            names=inference_response_keys,\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"Example output:\")\n",
    "predictions[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there may be *different costs* associated with *different kinds of error*, there are several ways to describe the performance of classifier models. Here we'll use some metrics and utilities provided by the `sklearn.metrics` library to drill in to our prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert messy text target value to true boolean:\n",
    "labels = test_data[target_attribute_name].apply(\n",
    "    lambda row: True if row == target_attribute_true_value else False\n",
    ")\n",
    "\n",
    "# Calculate candidate AUC and other basic metrics:\n",
    "for prediction, candidate in zip(predictions, candidates):\n",
    "    roc_auc = skmetrics.roc_auc_score(labels, prediction[\"probability\"])\n",
    "    ap = skmetrics.average_precision_score(labels, prediction[\"probability\"])\n",
    "    print(\n",
    "        \"%s's ROC AUC = %.2f, Average Precision = %.2f\" % (candidate[\"CandidateName\"], roc_auc, ap)\n",
    "    )\n",
    "    print(\n",
    "        skmetrics.classification_report(\n",
    "            test_data[target_attribute_name],\n",
    "            prediction[\"predicted_label\"],\n",
    "        )\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the [Receiver Operating Characteristic (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve for the model candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_tpr = []\n",
    "for prediction in predictions:\n",
    "    fpr, tpr, _ = skmetrics.roc_curve(labels, prediction[\"probability\"])\n",
    "    fpr_tpr.append(fpr)\n",
    "    fpr_tpr.append(tpr)\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 9), dpi=160, facecolor=\"w\", edgecolor=\"k\")\n",
    "plt.plot(*fpr_tpr)\n",
    "plt.legend([candidate[\"CandidateName\"] for candidate in candidates], loc=\"lower right\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the [precision-recall curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) for the model candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall = []\n",
    "for prediction in predictions:\n",
    "    precision, recall, _ = skmetrics.precision_recall_curve(labels, prediction[\"probability\"])\n",
    "    precision_recall.append(recall)\n",
    "    precision_recall.append(precision)\n",
    "\n",
    "plt.figure(num=None, figsize=(16, 9), dpi=160, facecolor=\"w\", edgecolor=\"k\")\n",
    "plt.plot(*precision_recall)\n",
    "plt.legend([candidate[\"CandidateName\"] for candidate in candidates], loc=\"lower left\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control costs, perhaps we'd like to select a minimum target **[precision](https://en.wikipedia.org/wiki/Precision_and_recall)** (percentage of customers offered an incentive who would actually have gone on to churn) and find the model that can yield the best **recall** (percentage of churning customers offered a retention incentive) at that operating point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_min_precision = 0.75\n",
    "\n",
    "best_recall = 0\n",
    "best_candidate_idx = -1\n",
    "best_candidate_threshold = -1\n",
    "candidate_idx = 0\n",
    "for prediction in predictions:\n",
    "    precision, recall, thresholds = skmetrics.precision_recall_curve(\n",
    "        labels, prediction[\"probability\"],\n",
    "    )\n",
    "    threshold_idx = np.argmax(precision >= target_min_precision)\n",
    "    if recall[threshold_idx] > best_recall:\n",
    "        best_recall = recall[threshold_idx]\n",
    "        best_candidate_threshold = thresholds[threshold_idx]\n",
    "        best_candidate_idx = candidate_idx\n",
    "    candidate_idx += 1\n",
    "\n",
    "print(\"Best Candidate Name: {}\".format(candidates[best_candidate_idx][\"CandidateName\"]))\n",
    "print(\"Best Candidate Threshold (Operation Point): {}\".format(best_candidate_threshold))\n",
    "print(\"Best Candidate Recall: {}\".format(best_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions of the best model based on the selected operating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_default = \\\n",
    "    predictions[best_candidate_idx][\"predicted_label\"] == target_attribute_true_value\n",
    "prediction_updated = predictions[best_candidate_idx][\"probability\"] >= best_candidate_threshold\n",
    "\n",
    "print(\n",
    "    \"Default Operating Point: recall={}, precision={}\".format(\n",
    "        skmetrics.recall_score(labels, prediction_default),\n",
    "        skmetrics.precision_score(labels, prediction_default)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Updated Operating Point: recall={}, precision={}\".format(\n",
    "        skmetrics.recall_score(labels, prediction_updated),\n",
    "        skmetrics.precision_score(labels, prediction_updated)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Selected Candidate\n",
    "\n",
    "After performing the analysis above, we can deploy the candidate that provides the best recall. We will use the `deploy` method to create the online inference endpoint. We'll use the same `inference_response_keys` from out batch transform jobs, but you can customize this as you wish. If `inference_response_keys` is not specified, only the `'predicted_label'` will be returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_response_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "predictor = automl.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    candidate=candidates[best_candidate_idx],\n",
    "    inference_response_keys=inference_response_keys,\n",
    "    predictor_cls=Predictor,\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=CSVDeserializer(),\n",
    ")\n",
    "\n",
    "print(\"Created endpoint: {}\".format(predictor.endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created our endpoint, we can send real-time predictions to the endpoint. The inference output will contain the model's`predicted_label` and `probability`. We use the custom threshold calculated above to determine our own `custom_predicted_label` based on the probability score in the inference response. If a `probability` is less than the `best_candidate_threshold`, the `custom_predicted_label` is the `False.` class. If a `probability` is greater than of equal to the `best_candidate_threshold`, the `custom_predicted_label` is the `True.` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_candidate_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prediction = predictor.predict(test_data_no_target.to_csv(sep=\",\", header=False, index=False))\n",
    "prediction_df = pd.DataFrame(prediction, columns=inference_response_keys)\n",
    "custom_predicted_labels = prediction_df.iloc[:, 1].astype(float).values >= best_candidate_threshold\n",
    "prediction_df[\"custom_predicted_label\"] = custom_predicted_labels\n",
    "prediction_df[\"custom_predicted_label\"] = prediction_df[\"custom_predicted_label\"].map(\n",
    "    {\n",
    "        False: target_attribute_values_counts.index[0],\n",
    "        True: target_attribute_values_counts.index[1],\n",
    "    }\n",
    ")\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup <a name=\"Cleanup\"></a>\n",
    "\n",
    "The Autopilot job creates many underlying artifacts such as dataset splits, preprocessing scripts, or preprocessed data, etc. This code, when un-commented, deletes them. This operation deletes all the generated models and the auto-generated notebooks as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.resource('s3')\n",
    "# s3_bucket = s3.Bucket(bucket)\n",
    "\n",
    "# s3_bucket.objects.filter(Prefix=prefix).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the created SageMaker Models and the deployed SageMaker Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "# predictor.delete_model()\n",
    "\n",
    "# for transformer in transformers:\n",
    "#     transformer.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
